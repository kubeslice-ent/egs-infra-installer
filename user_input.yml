---
################################################################################
# USER INPUT CONFIGURATION
# Last updated: JUNE 3, 2025
################################################################################

###############################################################################
# KUBERNETES DEPLOYMENT CONFIGURATION
# Controls whether to deploy a new Kubernetes cluster using Kubespray
###############################################################################
kubernetes_deployment:
  # Master switch for Kubernetes deployment
  enabled: false                           # Enable kubernetes deployment
  
  # API Server Configuration
  # Defines how clients will connect to the Kubernetes API server
  api_server:
    host: "PUBLIC_IP"  # Reference global IP
    port: 6443              # The port for the Kubernetes API server
                           # 6443 is the default secure port for Kubernetes
    secure: true           # Whether to use HTTPS for API server connections
                          # Should always be true in production environments

  # SSH Access Configuration
  # Required for Kubespray to access and configure nodes
  ssh_key_path: "/absolute/path/to/.ssh/k8s_rsa"  # Full path to the SSH private key file
                                            # Used to authenticate with the cluster nodes
  default_ansible_user: "REPLACE_SSH_USER"         # Default SSH user for all nodes
                                           # Common values: ubuntu (Ubuntu), ec2-user (AWS)
  # Ansible Sudo Password Configuration
  # Optional: Can be set here, via ANSIBLE_SUDO_PASS environment variable,
  # or will prompt if neither is set
  ansible_sudo_pass: ""  # Leave empty to use environment variable or prompt

  # Control Plane Node Configuration
  # Defines the master nodes that will run the Kubernetes control plane
  control_plane_nodes:
    - name: "master-1"                # Unique identifier for the control plane node
      ansible_host: "PUBLIC_IP"  # Reference global IP
      ansible_user: "REPLACE_SSH_USER"      # SSH username for this specific node
                                     # Overrides default_ansible_user if different
      ansible_become: true            # Enable privilege escalation (sudo)
      ansible_become_method: "sudo"     # Method for privilege escalation
      ansible_become_user: "root"       # Target user for privilege escalation
      private_ip: "PRIVATE_IP"      # Internal/private IP address
                                     # Used for internal cluster communication
  # Kubernetes Network Configuration
  # Defines the network ranges for various Kubernetes components
  network_config:
    service_subnet: "10.233.0.0/18"  # CIDR range for Kubernetes services
                                    # Must not overlap with pod_subnet or node network
    pod_subnet: "10.233.64.0/18"    # CIDR range for Kubernetes pods
                                   # Must not overlap with service_subnet or node network
    node_prefix: 24                 # Subnet mask for node network
                                  # /24 allows 256 IP addresses per subnet

  # Firewall Configuration
  # Controls access to cluster nodes
  firewall:
    enabled: true                            # Whether to configure firewall rules
    allow_additional_ports:
      - "80"
      - "443"
                                           # Added 80,443 for nginx-ingress

  # NVIDIA Container Runtime Configuration
  # Required for GPU support in the cluster
  nvidia_runtime:
    enabled: true                           # Enable NVIDIA container runtime
                                          # Set to true if using NVIDIA GPUs
    install_toolkit: true                   # Install NVIDIA Container Toolkit
                                          # Required for GPU support
    configure_containerd: true              # Configure containerd for NVIDIA runtime
    create_runtime_class: true              # Create Kubernetes RuntimeClass for NVIDIA

  # Kubernetes Component Configuration
  # Core Kubernetes infrastructure choices
  network_plugin: "calico"                  # Container Network Interface (CNI) choice
                                          # Options: calico, flannel, weave, cilium
  
  container_runtime: "containerd"           # Container runtime for Kubernetes
                                          # containerd is the current standard
  
  dns_mode: "coredns"                      # DNS service for the cluster
                                         # CoreDNS is the default since Kubernetes 1.13
  # Async Execution Configuration
  # Controls timeout and polling for long-running tasks
  async_config:
    timeout: 3600                          # Maximum time (in seconds) to wait for task completion
    poll_interval: 5                       # How often (in seconds) to check task status

###############################################################################
# REQUIRED SETTINGS
# These settings must be configured regardless of deployment type
###############################################################################

# Validation and Execution Settings
validate_prerequisites:
  enabled: true                              # Required: Must be enabled for validation

execution_order_enabled: true                # Required: Must be enabled for ordered execution

# Required Kubeconfig Settings
global_control_plane_ip: "PUBLIC_IP"         # Provide the public IP for metallb/Nginx
global_kubeconfig: "files/kubeconfig"        # Required: Path to kubeconfig file
global_kubecontext: "kubernetes-admin@cluster.local"  # Required: Kubernetes context
use_global_context: true                     # Required: Use global context

# Required Helm Settings
use_local_charts: false                      # Required: Use local Helm charts
local_charts_path: "charts"                  # Required: Path to local charts
global_chart_repo_url: ""                    # Required: Global Helm repository URL
global_repo_username: ""                     # Required: Repo username if using private repos
global_repo_password: ""                     # Required: Repo password if using private repos
readd_helm_repos: true                       # Required: Re-add Helm repos

# Required Credentials
ngc_api_key: "{{ lookup('env', 'NGC_API_KEY') }}"              # Required for NGC access
ngc_docker_api_key: "{{ lookup('env', 'NGC_DOCKER_API_KEY') }}" # Required for NGC Docker registry
avesha_docker_username: "{{ lookup('env', 'AVESHA_DOCKER_USERNAME') }}" # Required for Avesha images
avesha_docker_password: "{{ lookup('env', 'AVESHA_DOCKER_PASSWORD') }}" # Required for Avesha images

# Required Execution Order
execution_order:
  # Core Infrastructure
  - metallb_chart                 # Install MetalLB Helm chart first
  - metallb_l2_config            # Configure MetalLB L2 mode
  - metallb_ip_pool              # Configure IP address pool
  - nginx_ingress_config         # Apply Nginx Ingress configuration
  - nginx_ingress_chart          # Install Nginx Ingress Controller

  # Existing Applications
  - gpu_operator_chart
  - prometheus_stack
  - pushgateway_manifest
  - keda_chart
  - nim_operator_chart
  - create_ngc_secrets
  - verify_ngc_secrets
  - create_avesha_secret

  # NIM 1B Components
  - nim_cache_manifest_1b
  - nim_service_manifest_1b
  - keda_scaled_object_manifest_1b
  - create_inference_pod_configmap_1b
  - smart_scaler_inference_1b
  - create_locust_configmap_1b
  - locust_manifest_1b

  # NIM 8B Components
  - nim_cache_manifest_8b
  - nim_service_manifest_8b
  - keda_scaled_object_manifest_8b
  - create_inference_pod_configmap_8b
  - smart_scaler_inference_8b
  - create_locust_configmap_8b
  - locust_manifest_8b

  # NIM 70B Components
  - nim_cache_manifest_70b
  - nim_service_manifest_70b
  - keda_scaled_object_manifest_70b
  - create_inference_pod_configmap_70b
  - smart_scaler_inference_70b
  - create_locust_configmap_70b
  - locust_manifest_70b




###############################################################################
# OPTIONAL CONFIGURATION
###############################################################################

# MetalLB Configuration
metallb:
  enabled: true                          # Enable MetalLB
  namespace: "metallb"                   # Namespace for MetalLB deployment
  address_range: 172.18.1.1-172.18.1.16  # Reference global IP
  protocol: "layer2"                     # Use Layer 2 mode

# Nginx Ingress Configuration
nginx_ingress:
  enabled: true
  namespace: "ingress-nginx"
  replica_count: 1
  service_type: "LoadBalancer"
  metrics_enabled: true
  default_ssl_certificate: false
  enable_proxy_protocol: false
  enable_real_ip: true

# Optional Docker Registry Configuration
global_image_pull_secret:
  repository: "https://index.docker.io/v1/"  # Optional: Docker registry URL
  username: "<REGISTRY_USERNAME>"            # Optional: Registry username
  password: "<REGISTRY_PASSWORD>"            # Optional: Registry password

###############################################################################
# HELM CHARTS CONFIGURATION
###############################################################################

helm_charts:
  # MetalLB Chart Configuration
  metallb_chart:
    release_name: "metallb"
    chart_ref: "metallb"
    release_namespace: "{{ metallb.namespace }}"
    create_namespace: true
    wait: true
    timeout: 600  # Increased timeout
    dependencies: []  # No dependencies for first chart
    chart_repo_url: "https://metallb.github.io/metallb"
    chart_version: "0.15.0"
    release_values:
      prometheus:
        serviceMonitor:
          enabled: false
      controller:
        logLevel: info
      speaker:
        logLevel: info
        frr:
          enabled: false
      webhookConfiguration:
        enabled: true
        timeoutSeconds: 30
      crds:
        enabled: true
      rbac:
        create: true

  # Nginx Ingress Controller Chart
  nginx_ingress_chart:
    release_name: "ingress-nginx"
    chart_ref: "ingress-nginx"
    release_namespace: "ingress-nginx"
    create_namespace: true
    wait: true
    timeout: 600
    dependencies:
      - metallb_chart     # Only depend on MetalLB chart
    chart_repo_url: "https://kubernetes.github.io/ingress-nginx"
    chart_repo_name: "ingress-nginx"
    chart_version: "4.7.1"
    release_values:
      controller:
        replicaCount: 1
        service:
          enabled: true
          type: "LoadBalancer"
        metrics:
          enabled: true
          serviceMonitor:
            enabled: false
        config:
          use-proxy-protocol: false
          real-ip-header: "proxy_protocol"
          proxy-real-ip-cidr: "0.0.0.0/0"
          use-forwarded-headers: "true"
        publishService:
          enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
            timeoutSeconds: 30
        resources:
          requests:
            cpu: "100m"
            memory: "90Mi"
          limits:
            cpu: "200m"
            memory: "512Mi"
        readinessProbe:
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
      defaultBackend:
        enabled: true
        service:
          type: ClusterIP
        resources:
          requests:
            cpu: "10m"
            memory: "20Mi"
          limits:
            cpu: "20m"
            memory: "40Mi"

  gpu_operator_chart:
    release_name: "gpu-operator"
    chart_ref: "gpu-operator"
    release_namespace: "gpu-operator"
    create_namespace: true
    wait: true
    chart_version: "v25.3.0"
    release_values:
      mig:
        strategy: "none"
      dcgm:
        enabled: true
      driver:
        enabled: false
      cdi:
        default: true
        enabled: true
      hostPaths:
        driverInstallDir: "/home/kubernetes/bin/nvidia"
      toolkit:
        installDir: "/home/kubernetes/bin/nvidia"
        env:
          - name: "CONTAINERD_RUNTIME_CLASS"
            value: "nvidia"
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    dependencies: []  # Can start independently


# OPTIONAL: Prometheus Stack
  prometheus_stack:
    release_name: "prometheus"
    chart_ref: "kube-prometheus-stack"
    release_namespace: "monitoring"
    create_namespace: true
    wait: true
    chart_repo_url: "https://prometheus-community.github.io/helm-charts"
    chart_version: "55.5.0"
    release_values:
      kubeEtcd:
        enabled: false
      prometheus:
        prometheusSpec:
          retention: "15d"
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: local-path
                accessModes: ["ReadWriteOnce"]
                resources:
                  requests:
                    storage: 1Gi
          additionalScrapeConfigs:
            - job_name: "gpu-metrics"
              scrape_interval: "1s"
              metrics_path: "/metrics"
              scheme: "http"
              kubernetes_sd_configs:
                - role: "endpoints"
                  namespaces:
                    names:
                      - "monitoring"
                      - "gpu-operator"
              relabel_configs:
                - source_labels: ["__meta_kubernetes_endpoints_name"]
                  action: "drop"
                  regex: ".*-node-feature-discovery-master"
                - source_labels: ["__meta_kubernetes_pod_node_name"]
                  action: "replace"
                  target_label: "kubernetes_node"
      prometheusOperator:
        enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
      kubelet:
        serviceMonitor:
          https: false
      grafana:
        enabled: true
        persistence:
          enabled: true
          size: "1Gi"
      defaultRules:
        rules:
          etcd: false
    dependencies: []  # Can start independently

  # OPTIONAL: KEDA Chart
  keda_chart:
    release_name: "keda"
    chart_ref: "keda"
    release_namespace: "keda"
    create_namespace: true
    wait: true
    chart_repo_url: "https://kedacore.github.io/charts"
    chart_version: "2.12.1"
    dependencies: 
      - prometheus_stack  # KEDA might need Prometheus for metrics

  # OPTIONAL: NIM Operator Chart
  nim_operator_chart:
    release_name: "nim"
    chart_ref: "k8s-nim-operator"
    release_namespace: "nim"
    create_namespace: true
    wait: true
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    chart_version: "v1.0.1"
    dependencies:
      - gpu_operator_chart  # NIM operator needs GPU operator

###############################################################################
# KUBERNETES MANIFESTS CONFIGURATION
###############################################################################

manifests:
  # MetalLB IP Pool Configuration
  metallb_ip_pool:
    name: "metallb-ip-pool"
    manifest_file: "files/metallb-ip-pool.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true

  # MetalLB L2 Advertisement Configuration
  metallb_l2_config:
    name: "metallb-l2-config"
    manifest_file: "files/metallb-l2-config.yaml.j2"
    namespace: "metallb"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    dependencies: 
      - metallb_chart  # Ensure MetalLB chart is fully installed first

  # Nginx Ingress Additional Configuration
  nginx_ingress_config:
    name: "nginx-ingress-config"
    manifest_file: "files/nginx-ingress-config.yaml.j2"
    namespace: "ingress-nginx"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 60
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    dependencies: []

  # OPTIONAL: Pushgateway Manifest
  pushgateway_manifest:
    name: "pushgateway-setup"
    manifest_file: "files/pushgateway.yaml.j2"
    namespace: "monitoring"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    variables:
      pushgateway_name: "pushgateway"
      pushgateway_namespace: "monitoring"
      pushgateway_replicas: 1
      pushgateway_image: "prom/pushgateway"
      pushgateway_image_pull_policy: "IfNotPresent"
      pushgateway_service_name: "pushgateway"
      pushgateway_service_type: "ClusterIP"
      pushgateway_service_protocol: "TCP"
      pushgateway_service_port: 9091
      pushgateway_service_target_port: 9091
      pushgateway_monitor_name: "pushgateway"
      pushgateway_monitor_release: "prometheus"
      pushgateway_monitor_path: "/metrics"
      pushgateway_monitor_interval: "5s"
      pushgateway_resources_requests_cpu: "100m"
      pushgateway_resources_requests_memory: "64Mi"
      pushgateway_resources_limits_cpu: "200m"
      pushgateway_resources_limits_memory: "128Mi"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true

 # NIM 1B Components
  nim_cache_manifest_1b:
    name: "nim-cache-setup_1b"
    manifest_file: "files/nim-cache.yaml_1b.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_cache_name: "meta-llama3-1b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.2-1b-instruct:1.8.5"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "tensorrt_llm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_1b:
    name: "nim-service-setup_1b"
    manifest_file: "files/nim-service.yaml_1b.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_service_name: "meta-llama3-1b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.2-1b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.5"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-1b-instruct"
      nim_service_storage_cache_profile: "" #TODO: change to 1b profile
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_1b:
    name: keda-scaled-object-setup_1b
    manifest_file: "files/keda-scaled-object.yaml_1b.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      keda_scaled_object_name: "llm-demo-keda-1b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-1b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: >-
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-1b-instruct"}

  smart_scaler_inference_1b:
    name: smart-scaler-inference-setup_1b
    manifest_file: "files/smart-scaler-inference.yaml_1b.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-1b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-1b"
        cluster_name: "nim-llama-1b"
        tenant_id: "tenant-b200-local-1b"
        app_name: "nim-llama-1b"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-1b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config-inference_1b.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_1b:
    name: locust-load-1b
    manifest_file: "files/locust-deploy_1b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      locust_name: "locust-load-1b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-1b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-1b"

 # NIM 1B Components
  nim_cache_manifest_8b:
    name: "nim-cache-setup_8b"
    manifest_file: "files/nim-cache.yaml_8b.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_cache_name: "meta-llama3-8b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.4"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "vllm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_8b:
    name: "nim-service-setup_8b"
    manifest_file: "files/nim-service.yaml_8b.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_service_name: "meta-llama3-8b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-8b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.4"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-8b-instruct"
      nim_service_storage_cache_profile: "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_8b:
    name: keda-scaled-object-setup_8b
    manifest_file: "files/keda-scaled-object.yaml_8b.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      keda_scaled_object_name: "llm-demo-keda-8b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-8b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: 
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-8b-instruct"}

  smart_scaler_inference_8b:
    name: smart-scaler-inference-setup_8b
    manifest_file: "files/smart-scaler-inference.yaml_8b.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-8b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-8b"
        cluster_name: "nim-llama-8b"
        tenant_id: "tenant-b200-local-8b"
        app_name: "nim-llama-8b"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-8b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config-inference_8b.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_8b:
    name: locust-load-8b
    manifest_file: "files/locust-deploy_8b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      locust_name: "locust-load-8b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-8b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-8b"


   # OPTIONAL: NIM Cache Manifest
  nim_cache_manifest_70b:
    name: "nim-cache-setup_70b"
    manifest_file: "files/nim-cache.yaml_70b.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_cache_name: "meta-llama3-70b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-70b-instruct:1.8.5"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "tensorrt_llm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "8b87146e39b0305ae1d73bc053564d1b4b4c565f81aa5abe3e84385544ca9b60"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest_70b:
    name: "nim-service-setup_70b"
    manifest_file: "files/nim-service.yaml_70b.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_service_name: "meta-llama3-70b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-70b-instruct"  # Test value to verify variable passing
      nim_service_image_tag: "1.8.5"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-70b-instruct"
      nim_service_storage_cache_profile: "8b87146e39b0305ae1d73bc053564d1b4b4c565f81aa5abe3e84385544ca9b60"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest_70b:
    name: keda-scaled-object-setup_70b
    manifest_file: "files/keda-scaled-object.yaml_70b.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      keda_scaled_object_name: "llm-demo-keda-70b"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-70b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: 
        smartscaler_hpa_num_pods{job="pushgateway", ss_deployment_name="meta-llama3-70b-instruct"}

  smart_scaler_inference_70b:
    name: smart-scaler-inference-setup_70b
    manifest_file: "files/smart-scaler-inference.yaml_70b.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      smart_scaler_name: "smart-scaler-llm-inf-70b"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app-70b"
        cluster_name: "nim-llama"
        tenant_id: "tenant-b200-local"
        app_name: "nim-llama"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config-70b"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config-inference_70b.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest_70b:
    name: locust-load-70b
    manifest_file: "files/locust-deploy_70b.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      locust_name: "locust-load-70b"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-70b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile-70b"

###############################################################################
# COMMAND EXECUTION CONFIGURATION
###############################################################################

command_exec:
  - name: "create_ngc_secrets"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      # First check if secrets exist
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim get secret ngc-secret >/dev/null 2>&1; then
            echo "Secret ngc-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim delete secret ngc-secret
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim create secret docker-registry ngc-secret \
            --docker-server=nvcr.io \
            --docker-username='$oauthtoken' \
            --docker-password="${NGC_DOCKER_API_KEY}" \
            --docker-email='your.email@solo.io'
        env:
          NGC_DOCKER_API_KEY: "{{ ngc_docker_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

      # Handle ngc-api-secret
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret ngc-api-secret -n nim >/dev/null 2>&1; then
            echo "Secret ngc-api-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              delete secret ngc-api-secret -n nim
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create secret generic ngc-api-secret -n nim\
            --from-literal=NGC_API_KEY="${NGC_API_KEY}"
        env:
          NGC_API_KEY: "{{ ngc_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_ngc_secrets"
    commands:
      - cmd: "kubectl get secret ngc-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true
      - cmd: "kubectl get secret ngc-api-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "create_avesha_secret"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # First check and create namespace if it doesn't exist
          if ! kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get namespace smart-scaler >/dev/null 2>&1; then
            echo "Creating namespace smart-scaler..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              create namespace smart-scaler
          fi

          # Then handle the secret
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get secret avesha-systems >/dev/null 2>&1; then
            echo "Secret avesha-systems exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete secret avesha-systems
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler create secret docker-registry avesha-systems \
            --docker-username="${AVESHA_DOCKER_USERNAME}" \
            --docker-password="${AVESHA_DOCKER_PASSWORD}"
        env:
          AVESHA_DOCKER_USERNAME: "{{ avesha_docker_username }}"
          AVESHA_DOCKER_PASSWORD: "{{ avesha_docker_password }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_avesha_secret"
    commands:
      - cmd: "kubectl get secret avesha-systems -n smart-scaler -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true


  - name: "create_inference_pod_configmap_1b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-1b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-1b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-1b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-1b --from-file=files/config-inference_1b.json

  - name: "create_inference_pod_configmap_8b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-8b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-8b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-8b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-8b --from-file=files/config-inference_8b.json

  - name: "create_inference_pod_configmap_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config-70b >/dev/null 2>&1; then
            echo "ConfigMap mesh-config-70b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config-70b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config-70b --from-file=files/config-inference_70b.json

  - name: "create_locust_configmap_1b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-1b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-1b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-1b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-1b --from-file=locustfile.py=files/locust_1b.py

  - name: "create_locust_configmap_8b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-8b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-8b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-8b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-8b --from-file=locustfile.py=files/locust_8b.py

  - name: "create_locust_configmap_70b"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile-70b >/dev/null 2>&1; then
            echo "ConfigMap locustfile-70b exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile-70b
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile-70b --from-file=locustfile.py=files/locust_70b.py