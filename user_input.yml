---
################################################################################
# USER INPUT CONFIGURATION
# Last updated: JUNE 3, 2025
################################################################################

###############################################################################
# KUBERNETES DEPLOYMENT CONFIGURATION
# Controls whether to deploy a new Kubernetes cluster using Kubespray
###############################################################################

kubernetes_deployment:
  # Master switch for Kubernetes deployment
  enabled: true  # Set to true to deploy a new Kubernetes cluster
  
  # API Server Configuration
  # Defines how clients will connect to the Kubernetes API server
  api_server:
    host: "<API_SERVER_IP>"  # The IP address or hostname where the API server will be accessible
                            # Usually the public IP of your first control plane node
    port: 6443              # The port for the Kubernetes API server
                           # 6443 is the default secure port for Kubernetes
    secure: true           # Whether to use HTTPS for API server connections
                          # Should always be true in production environments

  # SSH Access Configuration
  # Required for Kubespray to access and configure nodes
  ssh_key_path: "/path/to/your/ssh/key.pem"  # Full path to the SSH private key file
                                            # Used to authenticate with the cluster nodes
  default_ansible_user: "<SSH_USER>"         # Default SSH user for all nodes
                                           # Common values: ubuntu (Ubuntu), ec2-user (AWS)
  # Ansible Sudo Password Configuration
  # Optional: Can be set here, via ANSIBLE_SUDO_PASS environment variable,
  # or will prompt if neither is set
  ansible_sudo_pass: ""  # Leave empty to use environment variable or prompt

  # Control Plane Node Configuration
  # Defines the master nodes that will run the Kubernetes control plane
  control_plane_nodes:
    - name: "master-1"                # Unique identifier for the control plane node
      ansible_host: "<PUBLIC_IP>"     # Public IP address for SSH access
      ansible_user: "<SSH_USER>"      # SSH username for this specific node
                                     # Overrides default_ansible_user if different
      ansible_become: true            # Enable privilege escalation (sudo)
      ansible_become_method: sudo     # Method for privilege escalation
      ansible_become_user: root       # Target user for privilege escalation
      private_ip: "<PRIVATE_IP>"      # Internal/private IP address
                                     # Used for internal cluster communication

  # Kubernetes Network Configuration
  # Defines the network ranges for various Kubernetes components
  network_config:
    service_subnet: "10.233.0.0/18"  # CIDR range for Kubernetes services
                                    # Must not overlap with pod_subnet or node network
    pod_subnet: "10.233.64.0/18"    # CIDR range for Kubernetes pods
                                   # Must not overlap with service_subnet or node network
    node_prefix: 24                 # Subnet mask for node network
                                  # /24 allows 256 IP addresses per subnet

  # Firewall Configuration
  # Controls access to cluster nodes
  firewall:
    enabled: true                            # Whether to configure firewall rules
    allow_additional_ports: ["80", "443"]    # Extra ports to open in the firewall
                                           # Added 80,443 for nginx-ingress

  # NVIDIA Container Runtime Configuration
  # Required for GPU support in the cluster
  nvidia_runtime:
    enabled: true                           # Enable NVIDIA container runtime
                                          # Set to true if using NVIDIA GPUs
    install_toolkit: true                   # Install NVIDIA Container Toolkit
                                          # Required for GPU support
    configure_containerd: true              # Configure containerd for NVIDIA runtime
    create_runtime_class: true              # Create Kubernetes RuntimeClass for NVIDIA

  # Kubernetes Component Configuration
  # Core Kubernetes infrastructure choices
  network_plugin: "calico"                  # Container Network Interface (CNI) choice
                                          # Options: calico, flannel, weave, cilium
  
  container_runtime: "containerd"           # Container runtime for Kubernetes
                                          # containerd is the current standard
  
  dns_mode: "coredns"                      # DNS service for the cluster
                                         # CoreDNS is the default since Kubernetes 1.13
  # Async Execution Configuration
  # Controls timeout and polling for long-running tasks
  async_config:
    timeout: 3600                          # Maximum time (in seconds) to wait for task completion
    poll_interval: 5                       # How often (in seconds) to check task status

###############################################################################
# REQUIRED SETTINGS
# These settings must be configured regardless of deployment type
###############################################################################

# Validation and Execution Settings
validate_prerequisites:
  enabled: true                              # Required: Must be enabled for validation

execution_order_enabled: true                # Required: Must be enabled for ordered execution

# Required Kubeconfig Settings
global_kubeconfig: "files/kubeconfig"        # Required: Path to kubeconfig file
global_kubecontext: "kubernetes-admin@cluster.local"  # Required: Kubernetes context
use_global_context: true                     # Required: Use global context

# Required Helm Settings
use_local_charts: false                      # Required: Use local Helm charts
local_charts_path: "charts"                  # Required: Path to local charts
global_chart_repo_url: ""                    # Required: Global Helm repository URL
global_repo_username: ""                     # Required: Repo username if using private repos
global_repo_password: ""                     # Required: Repo password if using private repos
readd_helm_repos: true                       # Required: Re-add Helm repos

# Required Credentials
ngc_api_key: "{{ lookup('env', 'NGC_API_KEY') }}"              # Required for NGC access
ngc_docker_api_key: "{{ lookup('env', 'NGC_DOCKER_API_KEY') }}" # Required for NGC Docker registry
avesha_docker_username: "{{ lookup('env', 'AVESHA_DOCKER_USERNAME') }}" # Required for Avesha images
avesha_docker_password: "{{ lookup('env', 'AVESHA_DOCKER_PASSWORD') }}" # Required for Avesha images

# Required Execution Order
execution_order:
  - gpu_operator_chart
  - prometheus_stack
  - pushgateway_manifest
  - keda_chart
  - nim_operator_chart
  - create_ngc_secrets
  - verify_ngc_secrets
  - create_avesha_secret
  - nim_cache_manifest
  - nim_service_manifest
  - keda_scaled_object_manifest
  - create_inference_pod_configmap
  - smart_scaler_inference
  - create_locust_configmap
  - locust_manifest

###############################################################################
# OPTIONAL CONFIGURATION
# These settings can be modified based on your requirements
###############################################################################

# Optional Docker Registry Configuration
global_image_pull_secret:
  repository: "https://index.docker.io/v1/"  # Optional: Docker registry URL
  username: "<REGISTRY_USERNAME>"            # Optional: Registry username
  password: "<REGISTRY_PASSWORD>"            # Optional: Registry password

###############################################################################
# HELM CHARTS CONFIGURATION
###############################################################################

helm_charts:
  # OPTIONAL: GPU Operator Chart
  gpu_operator_chart:
    release_name: "gpu-operator"
    chart_ref: "gpu-operator"
    release_namespace: "gpu-operator"
    create_namespace: true
    wait: true
    chart_version: "v25.3.0"
    release_values:
      mig:
        strategy: "none"
      dcgm:
        enabled: true
      driver:
        enabled: false
      cdi:
        default: true
        enabled: true
      hostPaths:
        driverInstallDir: "/home/kubernetes/bin/nvidia"
      toolkit:
        installDir: "/home/kubernetes/bin/nvidia"
        env:
          - name: "CONTAINERD_RUNTIME_CLASS"
            value: "nvidia"
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"

  # OPTIONAL: Prometheus Stack
  prometheus_stack:
    release_name: "prometheus"
    chart_ref: "kube-prometheus-stack"
    release_namespace: "monitoring"
    create_namespace: true
    wait: true
    chart_repo_url: "https://prometheus-community.github.io/helm-charts"
    chart_version: "55.5.0"
    release_values:
      kubeEtcd:
        enabled: false
      prometheus:
        prometheusSpec:
          retention: "15d"
          additionalScrapeConfigs:
            - job_name: "gpu-metrics"
              scrape_interval: "1s"
              metrics_path: "/metrics"
              scheme: "http"
              kubernetes_sd_configs:
                - role: "endpoints"
                  namespaces:
                    names:
                      - "monitoring"
                      - "gpu-operator"
              relabel_configs:
                - source_labels: ["__meta_kubernetes_endpoints_name"]
                  action: "drop"
                  regex: ".*-node-feature-discovery-master"
                - source_labels: ["__meta_kubernetes_pod_node_name"]
                  action: "replace"
                  target_label: "kubernetes_node"
      prometheusOperator:
        enabled: true
        admissionWebhooks:
          enabled: true
          patch:
            enabled: true
      kubelet:
        serviceMonitor:
          https: false
      grafana:
        enabled: true
        persistence:
          enabled: true
          size: "1Gi"
      defaultRules:
        rules:
          etcd: false

  # OPTIONAL: KEDA Chart
  keda_chart:
    release_name: "keda"
    chart_ref: "keda"
    release_namespace: "keda"
    create_namespace: true
    wait: true
    chart_repo_url: "https://kedacore.github.io/charts"
    chart_version: "2.12.1"

  # OPTIONAL: NIM Operator Chart
  nim_operator_chart:
    release_name: "nim"
    chart_ref: "k8s-nim-operator"
    release_namespace: "nim"
    create_namespace: true
    wait: true
    chart_repo_url: "https://helm.ngc.nvidia.com/nvidia"
    chart_version: "v1.0.1"

###############################################################################
# KUBERNETES MANIFESTS CONFIGURATION
###############################################################################

manifests:
  # OPTIONAL: Pushgateway Manifest
  pushgateway_manifest:
    name: "pushgateway-setup"
    manifest_file: "files/pushgateway.yaml"
    namespace: "pushgateway-system"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    variables:
      namespace: "monitoring"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true

  # OPTIONAL: NIM Cache Manifest
  nim_cache_manifest:
    name: "nim-cache-setup"
    manifest_file: "files/nim-cache.yaml.j2"
    namespace: "nim"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: "Available"
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_cache_name: "meta-llama3-8b-instruct"
      nim_cache_namespace: "nim"
      nim_cache_runtime_class: "nvidia"
      nim_cache_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_cache_model_puller: "nvcr.io/nim/meta/llama-3.1-8b-instruct:1.8.4"
      nim_cache_pull_secret: "ngc-secret"
      nim_cache_auth_secret: "ngc-api-secret"
      nim_cache_model_engine: "vllm"
      nim_cache_tensor_parallelism: "1"
      nim_cache_qos_profile: "throughput"
      nim_cache_model_profiles:
        - "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_cache_pvc_create: true
      nim_cache_storage_class: "local-path"
      nim_cache_pvc_size: "200Gi"
      nim_cache_volume_access_mode: "ReadWriteOnce"
      nim_cache_resources: {}

  nim_service_manifest:
    name: nim-service-setup
    manifest_file: "files/nim-service.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      nim_service_name: "meta-llama3-8b-instruct"
      nim_service_namespace: "nim"
      nim_service_runtime_class: "nvidia"
      nim_service_env:
        - name: "LOG_LEVEL"
          value: "INFO"
        - name: "VLLM_LOG_LEVEL"
          value: "INFO"
        - name: "NIM_LOG_LEVEL"
          value: "INFO"
        - name: "OMP_NUM_THREADS"
          value: "8"
        - name: "MAX_NUM_SEQS"
          value: "128"
      nim_service_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      nim_service_image_repository: "nvcr.io/nim/meta/llama-3.1-8b-instruct"
      nim_service_image_tag: "1.8.4"
      nim_service_image_pull_policy: "IfNotPresent"
      nim_service_image_pull_secrets:
        - "ngc-secret"
      nim_service_auth_secret: "ngc-api-secret"
      nim_service_metrics:
        enabled: true
        service_monitor:
          additional_labels:
            release: "prometheus"
      nim_service_storage_cache_name: "meta-llama3-8b-instruct"
      nim_service_storage_cache_profile: "4f904d571fe60ff24695b5ee2aa42da58cb460787a968f1e8a09f5a7e862728d"
      nim_service_replicas: 1
      nim_service_resources:
        limits:
          nvidia.com/gpu: 1
      nim_service_expose_type: "ClusterIP"
      nim_service_expose_port: 8000

  keda_scaled_object_manifest:
    name: keda-scaled-object-setup
    manifest_file: "files/keda-scaled-object.yaml.j2"
    namespace: nim
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      keda_scaled_object_name: "llm-demo-keda"
      keda_scaled_object_namespace: "nim"
      keda_scaled_object_target_name: "meta-llama3-8b-instruct"
      keda_scaled_object_polling_interval: 30
      keda_scaled_object_min_replicas: 1
      keda_scaled_object_max_replicas: 8
      keda_scaled_object_prometheus_address: "http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090"
      keda_scaled_object_metric_name: "smartscaler_hpa_num_pods"
      keda_scaled_object_threshold: "1"
      keda_scaled_object_query: >-
        smartscaler_hpa_num_pods{job="pushgateway", kubernetes_pod_name="meta-llama3-8b-instruct->nim->nim-llama", ss_deployment_name="meta-llama3-8b-instruct"}

  smart_scaler_inference:
    name: smart-scaler-inference-setup
    manifest_file: "files/smart-scaler-inference.yaml.j2"
    namespace: smart-scaler
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 600
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      smart_scaler_name: "smart-scaler-llm-inf"
      smart_scaler_namespace: "smart-scaler"
      smart_scaler_labels:
        service: "inference-tenant-app"
        cluster_name: "nim-llama"
        tenant_id: "tenant-b200-local"
        app_name: "nim-llama"
        app_version: "1.0"
      smart_scaler_tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      smart_scaler_resources:
        requests:
          memory: "1.5Gi"
          cpu: "100m"
      smart_scaler_replicas: 1
      smart_scaler_automount_sa: true
      smart_scaler_restart_policy: "Always"
      smart_scaler_config_volume_name: "data"
      smart_scaler_config_map_name: "mesh-config"
      smart_scaler_container_name: "inference"
      smart_scaler_image: "aveshasystems/smart-scaler-llm-inference-benchmark:v1.0.0"
      smart_scaler_image_pull_policy: "IfNotPresent"
      smart_scaler_command: ["/bin/sh", "-c"]
      smart_scaler_args:
        - "wandb disabled && python policy/inference_script.py -c /data/config-inference.json --restore -p ./checkpoint_000052 --mode mesh --no-smartscalerdb --no-cpu-switch --inference-session sess-llama-3-1-14-May"
      smart_scaler_config_mount_path: "/data"
      smart_scaler_ports: [9900, 8265, 4321, 6379]
      smart_scaler_image_pull_secret: "avesha-systems"

  locust_manifest:
    name: locust-setup
    manifest_file: "files/locust-deploy.yaml.j2"
    namespace: nim-load-test
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    wait: true
    wait_timeout: 300
    wait_condition:
      type: Available
      status: "True"
    validate: true
    strict_validation: true
    variables:
      locust_name: "locust-load"
      locust_namespace: "nim-load-test"
      locust_replicas: 1
      locust_image: "locustio/locust:2.15.1"
      locust_target_host: "http://meta-llama3-8b-instruct.nim.svc.cluster.local:8000"
      locust_cpu_request: "1"
      locust_memory_request: "1Gi"
      locust_cpu_limit: "2"
      locust_memory_limit: "2Gi"
      locust_configmap_name: "locustfile"

###############################################################################
# COMMAND EXECUTION CONFIGURATION
###############################################################################

command_exec:
  - name: "create_ngc_secrets"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      # First check if secrets exist
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim get secret ngc-secret >/dev/null 2>&1; then
            echo "Secret ngc-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim delete secret ngc-secret
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim create secret docker-registry ngc-secret \
            --docker-server=nvcr.io \
            --docker-username='$oauthtoken' \
            --docker-password="${NGC_DOCKER_API_KEY}" \
            --docker-email='your.email@solo.io'
        env:
          NGC_DOCKER_API_KEY: "{{ ngc_docker_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

      # Handle ngc-api-secret
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get secret ngc-api-secret -n nim >/dev/null 2>&1; then
            echo "Secret ngc-api-secret exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              delete secret ngc-api-secret -n nim
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create secret generic ngc-api-secret -n nim\
            --from-literal=NGC_API_KEY="${NGC_API_KEY}"
        env:
          NGC_API_KEY: "{{ ngc_api_key }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_ngc_secrets"
    commands:
      - cmd: "kubectl get secret ngc-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true
      - cmd: "kubectl get secret ngc-api-secret -n nim -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "create_avesha_secret"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          # First check and create namespace if it doesn't exist
          if ! kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            get namespace smart-scaler >/dev/null 2>&1; then
            echo "Creating namespace smart-scaler..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              create namespace smart-scaler
          fi

          # Then handle the secret
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get secret avesha-systems >/dev/null 2>&1; then
            echo "Secret avesha-systems exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete secret avesha-systems
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler create secret docker-registry avesha-systems \
            --docker-username="${AVESHA_DOCKER_USERNAME}" \
            --docker-password="${AVESHA_DOCKER_PASSWORD}"
        env:
          AVESHA_DOCKER_USERNAME: "{{ avesha_docker_username }}"
          AVESHA_DOCKER_PASSWORD: "{{ avesha_docker_password }}"
          KUBECONFIG: "{{ kubeconfig | default(global_kubeconfig) }}"

  - name: "verify_avesha_secret"
    commands:
      - cmd: "kubectl get secret avesha-systems -n smart-scaler -o jsonpath={.metadata.name} --kubeconfig={{ global_kubeconfig }} --context={{ global_kubecontext }}"
        env:
          KUBECONFIG: "{{ global_kubeconfig }}"
          KUBECONTEXT: "{{ global_kubecontext }}"
        ignore_errors: true

  - name: "create_inference_pod_configmap"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n smart-scaler get configmap mesh-config >/dev/null 2>&1; then
            echo "ConfigMap mesh-config exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n smart-scaler delete configmap mesh-config
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace smart-scaler --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create configmap -n smart-scaler mesh-config --from-file=files/config-inference.json

  - name: "create_locust_configmap"
    kubeconfig: "{{ kubeconfig | default(global_kubeconfig) }}"
    kubecontext: "{{ kubecontext | default(global_kubecontext) }}"
    commands:
      - cmd: |
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            create namespace nim-load-test --dry-run=client -o yaml | \
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            apply -f -
          if kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test get configmap locustfile >/dev/null 2>&1; then
            echo "ConfigMap locustfile exists, replacing..."
            kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
              --context={{ kubecontext | default(global_kubecontext) }} \
              -n nim-load-test delete configmap locustfile
          fi
          kubectl --kubeconfig={{ kubeconfig | default(global_kubeconfig) }} \
            --context={{ kubecontext | default(global_kubecontext) }} \
            -n nim-load-test create configmap locustfile --from-file=locustfile.py=files/locust.py
